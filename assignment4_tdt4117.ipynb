{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298d865a",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 4: Embedding Models, Dense Retrieval, and RAG\n",
    "\n",
    "**Student names**: _Your_names_here_ <br>\n",
    "**Group number**: _Your_group_here_ <br>\n",
    "**Date**: _Submission Date_\n",
    "\n",
    "## Important notes\n",
    "Please carefully read the following notes and consider them for the assignment delivery. Submissions that do not fulfill these requirements will not be assessed and should be submitted again.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. The assignment must be delivered in ipynb format.\n",
    "3. The assignment must be typed. Handwritten assignments are not accepted.\n",
    "\n",
    "**Due date**: 26.10.2025 23:59\n",
    "\n",
    "In this assignment, you will:\n",
    "- Build a vector search index over a blog corpus using sentence embeddings\n",
    "- Implement dense retrieval (cosine similarity)\n",
    "- Use the vector index as the foundation for a simple Retrieval-Augmented Generation (RAG) chat system with evaluation on three queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf612534",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the blog files, provided in the folder: \n",
    "- `blogs-sample` (in the same directory as this notebook)\n",
    "\n",
    "Use only the blog files provided in the folder below. Each file contains multiple `<post>` elements. Treat **each `<post>` as a separate document**.\n",
    "\n",
    "**The code to parse files is not provided. Implement the loading yourself in 4.1.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d681c2",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 – Load and parse blog documents\n",
    "\n",
    "Load all XML files from `blogs-sample`, extract the text of each `<post>`, and store one string per document. Keep the raw text per post as the document text.\n",
    "\n",
    "You may experience some trouble parsing all lines in the files, but this is okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389dfee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Load and parse the blog posts into a list named `documents`.\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eda412",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 – Embedding Models\n",
    "\n",
    "Select and load a sentence embedding model (e.g., `sentence-transformers/all-MiniLM-L6-v2`) and compute embeddings for all documents.\n",
    "\n",
    "- Store document embeddings in a variable named `doc_embeddings`.\n",
    "- Ensure that the same model will be used for query encoding later.\n",
    "\n",
    "**Report**:\n",
    "- The embedding matrix shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Load a sentence embedding model and encode all documents into `doc_embeddings`.\n",
    "# You may use `sentence-transformers`. Report the embedding matrix shape.\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defe752",
   "metadata": {},
   "source": [
    "\n",
    "## 4.3 – Dense Retrieval\n",
    "\n",
    "Implement a cosine similarity search over `doc_embeddings` for a given query.\n",
    "\n",
    "- Write a function `dense_search(query: str, k: int = 5) -> list[int]` that returns the indices of the top-k documents.\n",
    "- Use the same embedding model to encode the query.\n",
    "- Use cosine similarity for ranking.\n",
    "\n",
    "**Report**:\n",
    "- Results for the provided query showing the indices of the top results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dd7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement dense retrieval using cosine similarity.\n",
    "# Function signature to implement:\n",
    "# def dense_search(query: str, k: int = 5) -> list[int]:\n",
    "\n",
    "# Your code here\n",
    "\n",
    "#Report\n",
    "print(dense_search(\"How do people feel about their jobs?\", k=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25e527",
   "metadata": {},
   "source": [
    "\n",
    "## 4.4 – Build a Vector Search Index\n",
    "\n",
    "Build a lightweight vector index structure to enable repeated querying efficiently.\n",
    "\n",
    "- You may reuse `doc_embeddings` directly or create an index structure. Ensure the index can return top-k document indices given a query vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d959a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Initialize a vector index over `doc_embeddings`\n",
    "# Keep code minimal. The goal is to enable fast top-k retrieval for repeated queries.\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b616725",
   "metadata": {},
   "source": [
    "\n",
    "## 4.5 – RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Implement a simple RAG pipeline that:\n",
    "1) Retrieves the top-k documents for a user query using your vector index.\n",
    "2) Builds a prompt that includes the query and the retrieved document snippets.\n",
    "3) Uses a text generation model (your choice) to produce an answer grounded in the retrieved snippets.\n",
    "\n",
    "- Implement a function `rag_answer(query: str, k: int = 5) -> str`.\n",
    "- Keep the prompt simple and state clearly that the model should rely on the provided context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Implement a minimal RAG pipeline.\n",
    "# Steps (sketch):\n",
    "# - Use `dense_search` to get top-k indices.\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ae3d2",
   "metadata": {},
   "source": [
    "## 4.6 – Evaluation\n",
    "\n",
    "Use the following queries for your evaluation. For each query:\n",
    "\n",
    "- Run `dense_search(query, k=5)` to retrieve relevant documents.\n",
    "- Use `rag_answer(query, k=5)` to generate an answer using the top-5 retrieved documents.\n",
    "\n",
    "**Queries:**\n",
    "1. How do people deal with breakups?\n",
    "2. What do bloggers write about their daily routines?\n",
    "3. How do people feel about their jobs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "queries = [\n",
    "    \"How do people deal with breakups?\",\n",
    "    \"What do bloggers write about their daily routines?\",\n",
    "    \"How do people feel about their jobs?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c576f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run and report your evaluation as described above.\n",
    "\n",
    "def run_batch_evaluation(queries, k=5):\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Q{i}: {query}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        top_k = dense_search(query, k=k)\n",
    "        print(f\"Top-{k} retrieved indices:\", top_k)\n",
    "        print(\"\\nTop retrieved snippets:\")\n",
    "        for idx in top_k:\n",
    "            snippet = documents[idx].replace(\"\\n\", \" \").strip()\n",
    "            print(f\"[{idx}] {snippet[:200]}...\\n\")\n",
    "\n",
    "        print(\"RAG answer:\\n\")\n",
    "        answer = rag_answer(query, k=k)\n",
    "        print(answer)\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Run the evaluation\n",
    "run_batch_evaluation(queries, k=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
